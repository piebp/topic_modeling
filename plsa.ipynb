{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7b6ace-71b0-4a77-97d5-6cad63f6c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import artm\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='.', data_format='bow_uci',\n",
    "                                        collection_name='kos', target_folder='kos_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7110c21-a2be-4753-b96c-25d582ca65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, ascii_letters\n",
    "def cleanAndSplit(doc: str) -> [str]:\n",
    "    nopunctuation = [character for character in doc1.lower() if character in ascii_letters or character == ' ']\n",
    "    return \"\".join(nopunctuation).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "800af6f3-355e-4734-9c86-fe43f9d56b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"\"\"\n",
    "If you can keep your head when all about you   \n",
    "    Are losing theirs and blaming it on you,   \n",
    "If you can trust yourself when all men doubt you,\n",
    "    But make allowance for their doubting too;   \n",
    "If you can wait and not be tired by waiting,\n",
    "    Or being lied about, don’t deal in lies,\n",
    "Or being hated, don’t give way to hating,\n",
    "    And yet don’t look too good, nor talk too wise:\n",
    "\"\"\"\n",
    "doc2 = \"\"\"\n",
    "If you can dream—and not make dreams your master;   \n",
    "    If you can think—and not make thoughts your aim;   \n",
    "If you can meet with Triumph and Disaster\n",
    "    And treat those two impostors just the same;   \n",
    "If you can bear to hear the truth you’ve spoken\n",
    "    Twisted by knaves to make a trap for fools,\n",
    "Or watch the things you gave your life to, broken,\n",
    "    And stoop and build ’em up with worn-out tools:\n",
    "\"\"\"\n",
    "doc3 = \"\"\"\n",
    "If you can make one heap of all your winnings\n",
    "    And risk it on one turn of pitch-and-toss,\n",
    "And lose, and start again at your beginnings\n",
    "    And never breathe a word about your loss;\n",
    "If you can force your heart and nerve and sinew\n",
    "    To serve your turn long after they are gone,   \n",
    "And so hold on when there is nothing in you\n",
    "    Except the Will which says to them: ‘Hold on!’\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1fe39df7-4492-45a9-b24a-ce023b87ae3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/85/y194gy196m5gh1smpgnk_7w80000gn/T/ipykernel_78879/196052880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dca1e446-526c-4e4c-8ef2-1f5a6889e51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in '/Users/igorpostoev/nltk_data/corpora/stopwords'>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "028197b0-a949-44f7-9459-17b09d00ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('voted-kaggle-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "645100a9-d181-4724-b577-35c03c49f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\b[^\\d\\W]+\\b'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "en_stop = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dbf5c35f-44cb-42d8-a6ee-55499612eb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    The datasets contains transactions made by cre...\n",
      "1    The ultimate Soccer database for data analysis...\n",
      "Name: Description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Description'].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "06cb2403-ad75-4064-82b8-5ae89107af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in df['Description'].iteritems():\n",
    "    # clean and tokenize document string\n",
    "    raw = str(i[1]).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "    #stopped_tokens_new = [raw for raw in stopped_tokens if not raw in en_stop]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n",
    "    \n",
    "    # remove word containing only single char\n",
    "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(new_lemma_tokens)\n",
    "print(len(texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "242b7608-484a-4453-bc7a-d8306623841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2885\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6516218b-07d3-45b4-8382-a481f5fb2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = texts\n",
    "vocab = list(set([word for doc in docs for word in doc]))\n",
    "#later - filter by tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b5d6da1b-0a97-4622-8e9d-4861d1641e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(docs)\n",
    "V = len(vocab)\n",
    "T = 5\n",
    "imax = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "cb816477-850d-4a47-92e2-79759b5e32ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c26ca35e-76bc-40dd-b3ec-d56694f0ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI = np.random.rand(V, T)\n",
    "THETA = np.random.rand(T, D)\n",
    "DW = [[doc.count(word) for word in vocab] for doc in docs]\n",
    "DW = np.array(DW)\n",
    "lens = np.array([len(d) for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b7f9ea5f-e108-4a93-965e-6cc130904e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(imax):\n",
    "    nwt = np.zeros((V, T))\n",
    "    ntd = np.zeros((T, D))\n",
    "    nt = np.zeros((T))\n",
    "    product = PHI.dot(THETA)\n",
    "    for d in range(D):\n",
    "        for w in range(V):\n",
    "            if product[w][d] == 0:\n",
    "                ptdw = 0\n",
    "            else:\n",
    "                ptdw = PHI[w] * THETA.T[d] / product[w][d] # 1 x T\n",
    "            ntdw = DW[d][w] * ptdw\n",
    "            nwt[w] += ntdw\n",
    "            ntd.T[d] += ntdw\n",
    "            nt += ntdw\n",
    "    PHI = nwt / nt # W X T\n",
    "    THETA = (ntd / lens) # T x D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2ed0219e-f642-472c-8af2-0efb5055a645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30922,)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PHI.T[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d01f9cf0-fb63-4349-a939-80e77b5dc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_max_indeces = np.argsort(PHI.T[0])[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9655eb98-e5ac-4ddc-b126-a0befbbc7c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15936, 12715,  8313, 15919, 23159, 20232, 30729, 24322, 10675,\n",
       "        4460])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PHI_max_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "00b0dc9e-e199-4f1c-ba09-1d6aaec50acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_semantic_core(topic_index: int, core_len: int):\n",
    "    print(f\"topic {topic_index}: \\n\")\n",
    "    PHI_max_indeces = np.argsort(PHI.T[topic_index])[::-1][:core_len]\n",
    "    id_to_word = dict([(index, word) for index, word in enumerate(vocab)])\n",
    "    [print(id_to_word[index]) for index in PHI_max_indeces]\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "72a2f2c5-0af7-430d-821f-bde6a71717eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: \n",
      "\n",
      "dataset\n",
      "data\n",
      "http\n",
      "name\n",
      "com\n",
      "type\n",
      "number\n",
      "context\n",
      "one\n",
      "bea\n",
      "\n",
      "\n",
      "topic 1: \n",
      "\n",
      "data\n",
      "time\n",
      "number\n",
      "csv\n",
      "name\n",
      "inspiration\n",
      "context\n",
      "http\n",
      "model\n",
      "information\n",
      "\n",
      "\n",
      "topic 2: \n",
      "\n",
      "context\n",
      "data\n",
      "csv\n",
      "file\n",
      "acknowledgement\n",
      "code\n",
      "time\n",
      "like\n",
      "content\n",
      "university\n",
      "\n",
      "\n",
      "topic 3: \n",
      "\n",
      "data\n",
      "dataset\n",
      "file\n",
      "content\n",
      "name\n",
      "inspiration\n",
      "information\n",
      "year\n",
      "com\n",
      "total\n",
      "\n",
      "\n",
      "topic 4: \n",
      "\n",
      "data\n",
      "contains\n",
      "information\n",
      "dataset\n",
      "content\n",
      "acknowledgement\n",
      "context\n",
      "state\n",
      "number\n",
      "type\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[show_semantic_core(index, 10) for index in range(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2f01ca38-a84c-47cc-8328-718a23287837",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2, 4, 6], [2, 4, 6], [2, 4, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "110b1e4a-965d-4d56-b176-445d430a8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[2, 4, 6], [2, 4, 6], [2, 4, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8c6b761f-59e2-445d-a878-56de9819aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93903340-d7ab-4217-bff0-aabf93435824",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = artm.LDA(num_topics=15, alpha=0.01, beta=0.001, cache_theta=True,\n",
    "               num_document_passes=5, dictionary=batch_vectorizer.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beacd431-1283-4a61-819f-5e31285e648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0b2cf0-2f45-4d89-9653-01d3d827093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ['bush', 'party', 'tax', 'president', 'campaign', 'political', 'state', 'court', 'republican', 'states']\n",
      "Topic #1: ['iraq', 'war', 'military', 'troops', 'iraqi', 'killed', 'soldiers', 'people', 'forces', 'general']\n",
      "Topic #2: ['november', 'poll', 'governor', 'house', 'electoral', 'account', 'senate', 'republicans', 'polls', 'contact']\n",
      "Topic #3: ['senate', 'republican', 'campaign', 'republicans', 'race', 'carson', 'gop', 'democratic', 'debate', 'oklahoma']\n",
      "Topic #4: ['election', 'bush', 'specter', 'general', 'toomey', 'time', 'vote', 'campaign', 'people', 'john']\n",
      "Topic #5: ['kerry', 'dean', 'edwards', 'clark', 'primary', 'democratic', 'lieberman', 'gephardt', 'john', 'iowa']\n",
      "Topic #6: ['race', 'state', 'democrats', 'democratic', 'party', 'candidates', 'ballot', 'nader', 'candidate', 'district']\n",
      "Topic #7: ['administration', 'bush', 'president', 'house', 'years', 'commission', 'republicans', 'jobs', 'white', 'bill']\n",
      "Topic #8: ['dean', 'campaign', 'democratic', 'media', 'iowa', 'states', 'union', 'national', 'unions', 'party']\n",
      "Topic #9: ['house', 'republican', 'million', 'delay', 'money', 'elections', 'committee', 'gop', 'democrats', 'republicans']\n",
      "Topic #10: ['november', 'vote', 'voting', 'kerry', 'senate', 'republicans', 'house', 'polls', 'poll', 'account']\n",
      "Topic #11: ['iraq', 'bush', 'war', 'administration', 'president', 'american', 'saddam', 'iraqi', 'intelligence', 'united']\n",
      "Topic #12: ['bush', 'kerry', 'poll', 'polls', 'percent', 'voters', 'general', 'results', 'numbers', 'polling']\n",
      "Topic #13: ['time', 'house', 'bush', 'media', 'herseth', 'people', 'john', 'political', 'white', 'election']\n",
      "Topic #14: ['bush', 'kerry', 'general', 'state', 'percent', 'john', 'states', 'george', 'bushs', 'voters']\n"
     ]
    }
   ],
   "source": [
    "top_tokens = lda.get_top_tokens(num_tokens=10)\n",
    "for i, token_list in enumerate(top_tokens):\n",
    "    print(\"Topic #{0}: {1}\".format(i, token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04fc5a9c-70cd-42ec-99c6-8d554e7697a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3001</th>\n",
       "      <th>3002</th>\n",
       "      <th>3003</th>\n",
       "      <th>3004</th>\n",
       "      <th>3005</th>\n",
       "      <th>3006</th>\n",
       "      <th>3007</th>\n",
       "      <th>3008</th>\n",
       "      <th>3009</th>\n",
       "      <th>3010</th>\n",
       "      <th>...</th>\n",
       "      <th>2991</th>\n",
       "      <th>2992</th>\n",
       "      <th>2993</th>\n",
       "      <th>2994</th>\n",
       "      <th>2995</th>\n",
       "      <th>2996</th>\n",
       "      <th>2997</th>\n",
       "      <th>2998</th>\n",
       "      <th>2999</th>\n",
       "      <th>3000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic_0</th>\n",
       "      <td>0.071573</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.064975</td>\n",
       "      <td>0.329290</td>\n",
       "      <td>0.042682</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.110372</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.133693</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.304049</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.034589</td>\n",
       "      <td>0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_1</th>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.096175</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.143722</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.179343</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_2</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.037427</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.769481</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046747</td>\n",
       "      <td>0.008441</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.520337</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.693860</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_3</th>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.359139</td>\n",
       "      <td>0.007767</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.405234</td>\n",
       "      <td>0.146971</td>\n",
       "      <td>0.109285</td>\n",
       "      <td>0.031778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_4</th>\n",
       "      <td>0.462315</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.009519</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.001811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_5</th>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.039472</td>\n",
       "      <td>0.066723</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.005010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_6</th>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.413233</td>\n",
       "      <td>0.425574</td>\n",
       "      <td>0.244290</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.110521</td>\n",
       "      <td>0.085259</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.027951</td>\n",
       "      <td>0.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_7</th>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.042318</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.421977</td>\n",
       "      <td>0.153726</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_8</th>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_9</th>\n",
       "      <td>0.038160</td>\n",
       "      <td>0.955370</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.211434</td>\n",
       "      <td>0.508272</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.019946</td>\n",
       "      <td>0.423294</td>\n",
       "      <td>0.642150</td>\n",
       "      <td>0.952100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.958424</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.223766</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.234543</td>\n",
       "      <td>0.297471</td>\n",
       "      <td>0.073092</td>\n",
       "      <td>0.000716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 3430 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             3001      3002      3003      3004      3005      3006      3007  \\\n",
       "topic_0  0.071573  0.000340  0.064975  0.329290  0.042682  0.000143  0.110372   \n",
       "topic_1  0.005121  0.000258  0.000278  0.000223  0.000398  0.000084  0.001283   \n",
       "topic_2  0.000078  0.037427  0.000186  0.000178  0.000492  0.769481  0.000168   \n",
       "topic_3  0.001420  0.000187  0.005598  0.359139  0.007767  0.000205  0.405234   \n",
       "topic_4  0.462315  0.003936  0.009519  0.011956  0.000661  0.000124  0.000257   \n",
       "topic_5  0.000110  0.000151  0.001382  0.039472  0.066723  0.000432  0.000144   \n",
       "topic_6  0.000447  0.000182  0.004930  0.000325  0.000607  0.000140  0.413233   \n",
       "topic_7  0.000324  0.000474  0.000343  0.000231  0.000377  0.000097  0.002202   \n",
       "topic_8  0.000149  0.000210  0.000440  0.005485  0.000598  0.000155  0.000120   \n",
       "topic_9  0.038160  0.955370  0.000451  0.211434  0.508272  0.000176  0.019946   \n",
       "\n",
       "             3008      3009      3010  ...      2991      2992      2993  \\\n",
       "topic_0  0.000344  0.000585  0.000337  ...  0.001335  0.000568  0.000612   \n",
       "topic_1  0.000294  0.000199  0.000178  ...  0.000197  0.000390  0.096175   \n",
       "topic_2  0.000211  0.000271  0.000214  ...  0.046747  0.008441  0.000115   \n",
       "topic_3  0.146971  0.109285  0.031778  ...  0.004812  0.000369  0.000410   \n",
       "topic_4  0.000325  0.000228  0.000577  ...  0.000597  0.000398  0.001485   \n",
       "topic_5  0.000719  0.000530  0.005010  ...  0.000700  0.000433  0.022157   \n",
       "topic_6  0.425574  0.244290  0.002977  ...  0.006587  0.000376  0.000465   \n",
       "topic_7  0.000283  0.000266  0.000671  ...  0.023712  0.000561  0.000194   \n",
       "topic_8  0.000374  0.000324  0.000637  ...  0.000276  0.000411  0.000706   \n",
       "topic_9  0.423294  0.642150  0.952100  ...  0.000303  0.958424  0.001431   \n",
       "\n",
       "             2994      2995      2996      2997      2998      2999      3000  \n",
       "topic_0  0.000105  0.133693  0.004082  0.304049  0.000268  0.034589  0.000489  \n",
       "topic_1  0.003757  0.143722  0.000193  0.025600  0.179343  0.000151  0.000695  \n",
       "topic_2  0.520337  0.000168  0.693860  0.000151  0.000146  0.000069  0.000234  \n",
       "topic_3  0.000127  0.010107  0.000186  0.002427  0.000654  0.000404  0.000501  \n",
       "topic_4  0.000106  0.000721  0.000210  0.000549  0.001168  0.000120  0.001811  \n",
       "topic_5  0.000887  0.000134  0.000302  0.000151  0.000913  0.000219  0.000681  \n",
       "topic_6  0.110521  0.085259  0.002226  0.001881  0.012912  0.027951  0.001180  \n",
       "topic_7  0.000104  0.042318  0.000108  0.421977  0.153726  0.000137  0.000306  \n",
       "topic_8  0.000415  0.000107  0.014165  0.000196  0.000346  0.000183  0.000508  \n",
       "topic_9  0.000085  0.223766  0.000486  0.234543  0.297471  0.073092  0.000716  \n",
       "\n",
       "[10 rows x 3430 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_theta().iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c691e4-be0c-408f-a347-c18f65e61b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export ARTM_SHARED_LIBRARY=/Users/igorpostoev/Projects/Postgraduate/bigartm/build/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de175ab0-b03a-4a6b-8f41-0075a592e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43eab34e-9a32-4e53-ba0e-4e1b9e40fd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macOS-12.6.2-arm64-arm-64bit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d832f6b-bdcf-4ff3-9184-e1cc0e0b25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export ARTM_SHARED_LIBRARY=/usr/local/lib/libartm.dylib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d69f1bd-1b90-4718-b7ea-aac6f5b1ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement artm (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for artm\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install artm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
